{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2 \n",
    "## Monica Yao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \n",
    "Since this problem is using sigmoid and relu as its activation functions, I defined my functions the same way as we did in class (from Lecture 7). I also defined the loss function and the derivative of the loss function. \n",
    "\n",
    "I also used multiple functions to do forward propagation, which will calculate the output this neural network gives. The param dictionary is for all the inputs that we have. The functions I wrote will also puts every weighted sum and output of each neuron into the dictionary, and spit out a new one along with the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid_np(a):\n",
    "    return 1/(1+np.exp(-a))\n",
    "\n",
    "def sigmoid_np_prime(a):\n",
    "    return sigmoid_np(a)*(1-sigmoid_np(a))\n",
    "\n",
    "def relu_np(z):\n",
    "    return np.maximum(0 ,z)\n",
    "\n",
    "def relu_np_prime(z):\n",
    "    return np.where(z <= 0, 0, 1)\n",
    "\n",
    "def weightedsum_np(A, W, b):\n",
    "    return np.matmul(W, A) + b\n",
    "\n",
    "def loss_function(y, yHat):\n",
    "    return -(y*np.log(yHat)+(1-y)*np.log(1-yHat))\n",
    "\n",
    "def loss_function_prime(y, yHat):\n",
    "    return -y/yHat + 1/(1-yHat) - y/(1-yHat)\n",
    "\n",
    "params = {\n",
    "    'x': np.array([[1.0], [-2.0], [3.0]]),\n",
    "    'W1': np.array([[1.0, 2.0, 3.0]]),\n",
    "    'b1': np.array([-4.0]),\n",
    "    'W2': np.array([[2.0], [8.0]]),\n",
    "    'b2': np.array([[6.0], [5.0]]),\n",
    "    'W3': np.array([[8.0, 2.0]]),\n",
    "    'b3': np.array([-120.0]),\n",
    "}\n",
    "\n",
    "# I calculate the output of layer1, which becomes the input of our next layer\n",
    "# This uses activation function relu\n",
    "def firstLayer(params):\n",
    "    params['layer1z'] = weightedsum_np(params['x'], params['W1'], params['b1'])\n",
    "    params['layer1a'] = relu_np(params['layer1z'])\n",
    "    return params\n",
    "\n",
    "# I calculate the output of layer2, which becomes the input of our next layer\n",
    "# This uses activation function relu\n",
    "def secondLayer(params):\n",
    "    params = firstLayer(params)\n",
    "    params['layer2z'] = weightedsum_np(params['layer1a'], params['W2'], params['b2'])\n",
    "    params['layer2a'] = relu_np(params['layer2z'])\n",
    "    return params   \n",
    "\n",
    "# I calculate the output\n",
    "# This uses activation function sigmoid\n",
    "def outputLayer(params):\n",
    "    params = secondLayer(params)\n",
    "    params['layer3z'] = weightedsum_np(params['layer2a'], params['W3'], params['b3'])\n",
    "    params['layer3a'] = sigmoid_np(params['layer3z'])\n",
    "    return params, params['layer3a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "We use backpropagation where we will use the chain rule to find the partial derivative of the loss function with respect to the weight from the first input to the first neuron in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for the weights at layer one\n",
      "dJ_dW1: [[ -3.8144935 ]\n",
      " [  7.62898701]\n",
      " [-11.44348051]]\n",
      "Error for the weights at first neuron at layer onelayer one\n",
      "dJ_dW1[0]: [-3.8144935]\n"
     ]
    }
   ],
   "source": [
    "# I declare variables for a new dictionary that has all the weights and outputs of each neuron, and the final output\n",
    "newParams, output = outputLayer(params)\n",
    "\n",
    "# Calculating error at the last layer\n",
    "dJ_dlayer3a = loss_function_prime(1, output) \n",
    "dJ_dlayer3z = np.matmul(dJ_dlayer3a, sigmoid_np_prime(newParams['layer3z'])) \n",
    "\n",
    "# Calculating the partial derivative of the layer with repect to the earlier layer\n",
    "dlayer3z_dlayer2z = relu_np_prime(newParams['layer2z']) \n",
    "dlayer2z_dlayer1z = relu_np_prime(newParams['layer1z']) \n",
    "\n",
    "# Using chain rule to calculate the error at the last layer\n",
    "dJ_dlayer2z = np.multiply(np.matmul(newParams['W3'].transpose(), dJ_dlayer3z), dlayer3z_dlayer2z) \n",
    "dJ_dlayer1z = np.multiply(np.matmul(newParams['W2'].transpose(), dJ_dlayer2z), dlayer2z_dlayer1z) \n",
    "\n",
    "# Calculating the error for the weights at layer 1 using the error at that layer\n",
    "dJ_dW1 = np.matmul(params['x'], dJ_dlayer1z)\n",
    "\n",
    "print('Error for the weights at layer one')\n",
    "print(f'dJ_dW1: {dJ_dW1}')\n",
    "print('Error for the weights at first neuron at layer onelayer one') \n",
    "print(f'dJ_dW1[0]: {dJ_dW1[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Differentiation\n",
    "We now use a numerical differentiation to make sure that our calculations are correct. We calculate the actual value of $J(y, \\hat{y})$ then find the partial derivative $\\frac{\\partial{J}}{{w_1}^{[1]}[1]}$ by the defition of the derivative by changing the weight by a small amount epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for the weights at first neuron at layer onelayer one\n",
      "dJ/dw1: [[-3.81443975]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the output yHat then we call our loss function to find the loss\n",
    "newParams, output = outputLayer(params)\n",
    "j_out = loss_function(1, output)\n",
    "\n",
    "# Clone params, add epsilon to the first weight of the first neuron of the first layer\n",
    "params_epsilon = dict(newParams)\n",
    "epsilon = .000001\n",
    "newParams['W1'][0][0] += epsilon\n",
    "\n",
    "# Calculating the output and loss with new weight\n",
    "_, epsilon_output = outputLayer(params_epsilon)\n",
    "jepsilon = loss_function(1, epsilon_output)\n",
    "\n",
    "# Calculating the partial derivative using definition\n",
    "dJ_dw1 = (jepsilon-j_out)/epsilon\n",
    "\n",
    "print('Error for the weights at first neuron at layer onelayer one') \n",
    "print(f'dJ/dw1: {dJ_dw1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two partial derivatives match up! They are both around $\\frac{\\partial{J}}{{w_1}^{[1]}[1]} = -3.814 $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
