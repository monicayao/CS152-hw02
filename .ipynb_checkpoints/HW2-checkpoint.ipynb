{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2 \n",
    "## Monica Yao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \n",
    "Since this problem is using sigmoid and relu as its activation functions, I defined my functions the same way as we did in class (from Lecture 7). I also defined the loss function and the derivative of the loss function. \n",
    "\n",
    "I also used multiple functions to do forward propagation, which will calculate the output this neural network gives. The param dictionary is for all the inputs that we have. The functions I wrote will also puts every weighted sum and output of each neuron into the dictionary, and spit out a new one along with the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid_np(a):\n",
    "    return 1/(1+np.exp(-a))\n",
    "\n",
    "def sigmoid_np_prime(a):\n",
    "    return sigmoid_np(a)*(1-sigmoid_np(a))\n",
    "\n",
    "def relu_np(z):\n",
    "    return np.maximum(0 ,z)\n",
    "\n",
    "def relu_np_prime(z):\n",
    "    return np.where(z <= 0, 0, 1)\n",
    "\n",
    "def weightedsum_np(A, W, b):\n",
    "    return np.matmul(W, A) + b\n",
    "\n",
    "def loss_function(y, yHat):\n",
    "    return -(y*np.log(yHat)+(1-y)*np.log(1-yHat))\n",
    "\n",
    "def loss_function_prime(y, yHat):\n",
    "    return -y/yHat + 1/(1-yHat) - y/(1-yHat)\n",
    "\n",
    "params = {\n",
    "    'x': np.array([[1.0], [-2.0], [3.0]]),\n",
    "    'W1': np.array([[1.0, 2.0, 3.0]]),\n",
    "    'b1': np.array([-4.0]),\n",
    "    'W2': np.array([[2.0], [8.0]]),\n",
    "    'b2': np.array([[6.0], [5.0]]),\n",
    "    'W3': np.array([[8.0, 2.0]]),\n",
    "    'b3': np.array([-120.0]),\n",
    "}\n",
    "\n",
    "# I calculate the output of layer1, which becomes the input of our next layer\n",
    "# This uses activation function relu\n",
    "def firstLayer(params):\n",
    "    params['layer1z'] = weightedsum_np(params['x'], params['W1'], params['b1'])\n",
    "    params['layer1a'] = relu_np(params['layer1z'])\n",
    "    return params\n",
    "\n",
    "# I calculate the output of layer2, which becomes the input of our next layer\n",
    "# This uses activation function relu\n",
    "def secondLayer(params):\n",
    "    params = firstLayer(params)\n",
    "    params['layer2z'] = weightedsum_np(params['layer1a'], params['W2'], params['b2'])\n",
    "    params['layer2a'] = relu_np(params['layer2z'])\n",
    "    return params   \n",
    "\n",
    "# I calculate the output\n",
    "# This uses activation function sigmoid\n",
    "def outputLayer(params):\n",
    "    params = secondLayer(params)\n",
    "    params['layer3z'] = weightedsum_np(params['layer2a'], params['W3'], params['b3'])\n",
    "    params['layer3a'] = sigmoid_np(params['layer3z'])\n",
    "    return params, params['layer3a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "We use backpropagation where we will use the chain rule to find the partial derivative of the loss function with respect to the weight from the first input to the first neuron in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for the weights at layer one\n",
      "dJ_dW1: [[ -3.8144935 ]\n",
      " [  7.62898701]\n",
      " [-11.44348051]]\n",
      "Error for the weights at first neuron at layer onelayer one\n",
      "dJ_dW1[0]: [-3.8144935]\n"
     ]
    }
   ],
   "source": [
    "# I declare variables for a new dictionary that has all the weights and outputs of each neuron, and the final output\n",
    "newParams, output = outputLayer(params)\n",
    "\n",
    "# Calculating error at the last layer\n",
    "dJ_dlayer3a = loss_function_prime(1, output) \n",
    "dJ_dlayer3z = np.matmul(dJ_dlayer3a, sigmoid_np_prime(newParams['layer3z'])) \n",
    "\n",
    "# Calculating the partial derivative of the layer with repect to the earlier layer\n",
    "dlayer3z_dlayer2z = relu_np_prime(newParams['layer2z']) \n",
    "dlayer2z_dlayer1z = relu_np_prime(newParams['layer1z']) \n",
    "\n",
    "# Using chain rule to calculate the error at the last layer\n",
    "dJ_dlayer2z = np.multiply(np.matmul(newParams['W3'].transpose(), dJ_dlayer3z), dlayer3z_dlayer2z) \n",
    "dJ_dlayer1z = np.multiply(np.matmul(newParams['W2'].transpose(), dJ_dlayer2z), dlayer2z_dlayer1z) \n",
    "\n",
    "# Calculating the error for the weights at layer 1 using the error at that layer\n",
    "dJ_dW1 = np.matmul(params['x'], dJ_dlayer1z)\n",
    "\n",
    "print('Error for the weights at layer one')\n",
    "print(f'dJ_dW1: {dJ_dW1}')\n",
    "print('Error for the weights at first neuron at layer onelayer one') \n",
    "print(f'dJ_dW1[0]: {dJ_dW1[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Differentiation\n",
    "We now use a numerical differentiation to make sure that our calculations are correct. We calculate the actual value of $J(y, \\hat{y})$ then find the partial derivative $\\frac{\\partial{J}}{{w_1}^{[1]}[1]}$ by the defition of the derivative by changing the weight by a small amount epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for the weights at first neuron at layer onelayer one\n",
      "dJ/dw1: [[-3.81443975]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the output yHat then we call our loss function to find the loss\n",
    "newParams, output = outputLayer(params)\n",
    "j_out = loss_function(1, output)\n",
    "\n",
    "# Clone params, add epsilon to the first weight of the first neuron of the first layer\n",
    "params_epsilon = dict(newParams)\n",
    "epsilon = .000001\n",
    "newParams['W1'][0][0] += epsilon\n",
    "\n",
    "# Calculating the output and loss with new weight\n",
    "_, epsilon_output = outputLayer(params_epsilon)\n",
    "jepsilon = loss_function(1, epsilon_output)\n",
    "\n",
    "# Calculating the partial derivative using definition\n",
    "dJ_dw1 = (jepsilon-j_out)/epsilon\n",
    "\n",
    "print('Error for the weights at first neuron at layer onelayer one') \n",
    "print(f'dJ/dw1: {dJ_dw1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two partial derivatives match up! They are both around $\\frac{\\partial{J}}{{w_1}^{[1]}[1]} = -3.814 $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "a) For $ tan(x) $, a good activation function would be a linear or cubic function. $ tan(x) $ has range from $-inf$ to $inf$, so a good activation function would also have range from $-inf$ to $inf$, so $ g(z) = z $ or $ g(z) = z^3 $ would be good. \n",
    "\n",
    "b) For $ sin(x) $, a good activation function would be tanh, a scaled sigmoid. $ sin(x) $ has range $[-1, 1]$ so a good activation function would have a similar range. $ g(z) = tanh(z) = \\frac {e^z - e^{-z}} {e^z + e^{-z}} $ which has range $(-1, 1)$\n",
    "\n",
    "c) For the estimates of selling price of a house, a good activation function would be the relu. That is because the selling price will never be negative, and can reach a very high price. Thus, relu $g(z) = max(0, z)$ is the most fitting because it has range $ [0, inf) $ \n",
    "\n",
    "d) For the probability that a currency image is a counterfeit, a good activation function would be the sigmoid, because the probability ranges from $[0, 1]$ and similarly, the sigmoid $ g(z) = \\frac {1} {1 + e^{-z}} $ it will have range anywhere from $(0, 1)$ \n",
    "\n",
    "e) For the audio recording as input and classifying the sound as instruments, we would use softmax. This problem is a multiclass classification problem, because it will sort the sound into different classes (instruments). Softmax $g(x) = \\frac{e^x}{\\Sigma e^j}$ is good for a vector of predictions because it normalizes the data, and it is perfect for this problem to classify the instruments.\n",
    "\n",
    "f) For deciding what major a student is based on their courses, a good activation function would be the sigmoid. The major that a student is in could be decided based a probability from $[0, 1]$ for each major. The sigmoid $ g(z) = \\frac {1} {1 + e^{-z}} $ has range anywhere from $(0, 1)$, and will serve as a good final layer in the classification problem. An alternative would be the softmax $g(x) = \\frac{e^x}{\\Sigma e^j}$, because it can classify the major based on the courses, and it is a good activation function for a multiclass classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "The dimensions are  \n",
    "$ W^{[1]} \\quad  1x3 $  \n",
    "$ W^{[2]} \\quad  2x1 $  \n",
    "$ W^{[3]} \\quad  1x2 $  \n",
    "$ b^{[1]} \\quad  1x1 $   \n",
    "$ b^{[2]} \\quad  2x1 $  \n",
    "$ b^{[3]} \\quad  1x1 $  \n",
    "$ Z^{[1]} \\quad  1x50 $  \n",
    "$ Z^{[2]} \\quad  2x50 $  \n",
    "$ Z^{[3]} \\quad  1x50 $  \n",
    "$ A^{[1]} \\quad  1x50 $  \n",
    "$ A^{[2]} \\quad  2x50 $  \n",
    "$ A^{[3]} \\quad  1x50 $  \n",
    "\n",
    "$ A^{[3]} $ represents the output of each of the batch. Each of the element in the column of the matrix represents a single output the neural network of __one__ batch. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
